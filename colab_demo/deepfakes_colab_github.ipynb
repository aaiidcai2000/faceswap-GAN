{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepfakes_colab_github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2oTGygjSdq",
        "colab_type": "code",
        "outputId": "78525458-a59a-41e1-f37b-dcd81e53b0ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXY2GsMdt3e",
        "colab_type": "code",
        "outputId": "551855c5-9a9b-45dd-86dc-804c24e8090d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!git clone --depth 1 https://github.com/deepfakes/faceswap.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'faceswap'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/286)\u001b[K\rremote: Counting objects:   1% (3/286)\u001b[K\rremote: Counting objects:   2% (6/286)\u001b[K\rremote: Counting objects:   3% (9/286)\u001b[K\rremote: Counting objects:   4% (12/286)\u001b[K\rremote: Counting objects:   5% (15/286)\u001b[K\rremote: Counting objects:   6% (18/286)\u001b[K\rremote: Counting objects:   7% (21/286)\u001b[K\rremote: Counting objects:   8% (23/286)\u001b[K\rremote: Counting objects:   9% (26/286)\u001b[K\rremote: Counting objects:  10% (29/286)\u001b[K\rremote: Counting objects:  11% (32/286)\u001b[K\rremote: Counting objects:  12% (35/286)\u001b[K\rremote: Counting objects:  13% (38/286)\u001b[K\rremote: Counting objects:  14% (41/286)\u001b[K\rremote: Counting objects:  15% (43/286)\u001b[K\rremote: Counting objects:  16% (46/286)\u001b[K\rremote: Counting objects:  17% (49/286)\u001b[K\rremote: Counting objects:  18% (52/286)\u001b[K\rremote: Counting objects:  19% (55/286)\u001b[K\rremote: Counting objects:  20% (58/286)\u001b[K\rremote: Counting objects:  21% (61/286)\u001b[K\rremote: Counting objects:  22% (63/286)\u001b[K\rremote: Counting objects:  23% (66/286)\u001b[K\rremote: Counting objects:  24% (69/286)\u001b[K\rremote: Counting objects:  25% (72/286)\u001b[K\rremote: Counting objects:  26% (75/286)\u001b[K\rremote: Counting objects:  27% (78/286)\u001b[K\rremote: Counting objects:  28% (81/286)\u001b[K\rremote: Counting objects:  29% (83/286)\u001b[K\rremote: Counting objects:  30% (86/286)\u001b[K\rremote: Counting objects:  31% (89/286)\u001b[K\rremote: Counting objects:  32% (92/286)\u001b[K\rremote: Counting objects:  33% (95/286)\u001b[K\rremote: Counting objects:  34% (98/286)\u001b[K\rremote: Counting objects:  35% (101/286)\u001b[K\rremote: Counting objects:  36% (103/286)\u001b[K\rremote: Counting objects:  37% (106/286)\u001b[K\rremote: Counting objects:  38% (109/286)\u001b[K\rremote: Counting objects:  39% (112/286)\u001b[K\rremote: Counting objects:  40% (115/286)\u001b[K\rremote: Counting objects:  41% (118/286)\u001b[K\rremote: Counting objects:  42% (121/286)\u001b[K\rremote: Counting objects:  43% (123/286)\u001b[K\rremote: Counting objects:  44% (126/286)\u001b[K\rremote: Counting objects:  45% (129/286)\u001b[K\rremote: Counting objects:  46% (132/286)\u001b[K\rremote: Counting objects:  47% (135/286)\u001b[K\rremote: Counting objects:  48% (138/286)\u001b[K\rremote: Counting objects:  49% (141/286)\u001b[K\rremote: Counting objects:  50% (143/286)\u001b[K\rremote: Counting objects:  51% (146/286)\u001b[K\rremote: Counting objects:  52% (149/286)\u001b[K\rremote: Counting objects:  53% (152/286)\u001b[K\rremote: Counting objects:  54% (155/286)\u001b[K\rremote: Counting objects:  55% (158/286)\u001b[K\rremote: Counting objects:  56% (161/286)\u001b[K\rremote: Counting objects:  57% (164/286)\u001b[K\rremote: Counting objects:  58% (166/286)\u001b[K\rremote: Counting objects:  59% (169/286)\u001b[K\rremote: Counting objects:  60% (172/286)\u001b[K\rremote: Counting objects:  61% (175/286)\u001b[K\rremote: Counting objects:  62% (178/286)\u001b[K\rremote: Counting objects:  63% (181/286)\u001b[K\rremote: Counting objects:  64% (184/286)\u001b[K\rremote: Counting objects:  65% (186/286)\u001b[K\rremote: Counting objects:  66% (189/286)\u001b[K\rremote: Counting objects:  67% (192/286)\u001b[K\rremote: Counting objects:  68% (195/286)\u001b[K\rremote: Counting objects:  69% (198/286)\u001b[K\rremote: Counting objects:  70% (201/286)\u001b[K\rremote: Counting objects:  71% (204/286)\u001b[K\rremote: Counting objects:  72% (206/286)\u001b[K\rremote: Counting objects:  73% (209/286)\u001b[K\rremote: Counting objects:  74% (212/286)\u001b[K\rremote: Counting objects:  75% (215/286)\u001b[K\rremote: Counting objects:  76% (218/286)\u001b[K\rremote: Counting objects:  77% (221/286)\u001b[K\rremote: Counting objects:  78% (224/286)\u001b[K\rremote: Counting objects:  79% (226/286)\u001b[K\rremote: Counting objects:  80% (229/286)\u001b[K\rremote: Counting objects:  81% (232/286)\u001b[K\rremote: Counting objects:  82% (235/286)\u001b[K\rremote: Counting objects:  83% (238/286)\u001b[K\rremote: Counting objects:  84% (241/286)\u001b[K\rremote: Counting objects:  85% (244/286)\u001b[K\rremote: Counting objects:  86% (246/286)\u001b[K\rremote: Counting objects:  87% (249/286)\u001b[K\rremote: Counting objects:  88% (252/286)\u001b[K\rremote: Counting objects:  89% (255/286)\u001b[K\rremote: Counting objects:  90% (258/286)\u001b[K\rremote: Counting objects:  91% (261/286)\u001b[K\rremote: Counting objects:  92% (264/286)\u001b[K\rremote: Counting objects:  93% (266/286)\u001b[K\rremote: Counting objects:  94% (269/286)\u001b[K\rremote: Counting objects:  95% (272/286)\u001b[K\rremote: Counting objects:  96% (275/286)\u001b[K\rremote: Counting objects:  97% (278/286)\u001b[K\rremote: Counting objects:  98% (281/286)\u001b[K\rremote: Counting objects:  99% (284/286)\u001b[K\rremote: Counting objects: 100% (286/286)\u001b[K\rremote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/249)\u001b[K\rremote: Compressing objects:   1% (3/249)\u001b[K\rremote: Compressing objects:   2% (5/249)\u001b[K\rremote: Compressing objects:   3% (8/249)\u001b[K\rremote: Compressing objects:   4% (10/249)\u001b[K\rremote: Compressing objects:   5% (13/249)\u001b[K\rremote: Compressing objects:   6% (15/249)\u001b[K\rremote: Compressing objects:   7% (18/249)\u001b[K\rremote: Compressing objects:   8% (20/249)\u001b[K\rremote: Compressing objects:   9% (23/249)\u001b[K\rremote: Compressing objects:  10% (25/249)\u001b[K\rremote: Compressing objects:  11% (28/249)\u001b[K\rremote: Compressing objects:  12% (30/249)\u001b[K\rremote: Compressing objects:  13% (33/249)\u001b[K\rremote: Compressing objects:  14% (35/249)\u001b[K\rremote: Compressing objects:  15% (38/249)\u001b[K\rremote: Compressing objects:  16% (40/249)\u001b[K\rremote: Compressing objects:  17% (43/249)\u001b[K\rremote: Compressing objects:  18% (45/249)\u001b[K\rremote: Compressing objects:  19% (48/249)\u001b[K\rremote: Compressing objects:  20% (50/249)\u001b[K\rremote: Compressing objects:  21% (53/249)\u001b[K\rremote: Compressing objects:  22% (55/249)\u001b[K\rremote: Compressing objects:  23% (58/249)\u001b[K\rremote: Compressing objects:  24% (60/249)\u001b[K\rremote: Compressing objects:  25% (63/249)\u001b[K\rremote: Compressing objects:  26% (65/249)\u001b[K\rremote: Compressing objects:  27% (68/249)\u001b[K\rremote: Compressing objects:  28% (70/249)\u001b[K\rremote: Compressing objects:  29% (73/249)\u001b[K\rremote: Compressing objects:  30% (75/249)\u001b[K\rremote: Compressing objects:  31% (78/249)\u001b[K\rremote: Compressing objects:  32% (80/249)\u001b[K\rremote: Compressing objects:  33% (83/249)\u001b[K\rremote: Compressing objects:  34% (85/249)\u001b[K\rremote: Compressing objects:  35% (88/249)\u001b[K\rremote: Compressing objects:  36% (90/249)\u001b[K\rremote: Compressing objects:  37% (93/249)\u001b[K\rremote: Compressing objects:  38% (95/249)\u001b[K\rremote: Compressing objects:  39% (98/249)\u001b[K\rremote: Compressing objects:  40% (100/249)\u001b[K\rremote: Compressing objects:  41% (103/249)\u001b[K\rremote: Compressing objects:  42% (105/249)\u001b[K\rremote: Compressing objects:  43% (108/249)\u001b[K\rremote: Compressing objects:  44% (110/249)\u001b[K\rremote: Compressing objects:  45% (113/249)\u001b[K\rremote: Compressing objects:  46% (115/249)\u001b[K\rremote: Compressing objects:  47% (118/249)\u001b[K\rremote: Compressing objects:  48% (120/249)\u001b[K\rremote: Compressing objects:  49% (123/249)\u001b[K\rremote: Compressing objects:  50% (125/249)\u001b[K\rremote: Compressing objects:  51% (127/249)\u001b[K\rremote: Compressing objects:  52% (130/249)\u001b[K\rremote: Compressing objects:  53% (132/249)\u001b[K\rremote: Compressing objects:  54% (135/249)\u001b[K\rremote: Compressing objects:  55% (137/249)\u001b[K\rremote: Compressing objects:  56% (140/249)\u001b[K\rremote: Compressing objects:  57% (142/249)\u001b[K\rremote: Compressing objects:  58% (145/249)\u001b[K\rremote: Compressing objects:  59% (147/249)\u001b[K\rremote: Compressing objects:  60% (150/249)\u001b[K\rremote: Compressing objects:  61% (152/249)\u001b[K\rremote: Compressing objects:  62% (155/249)\u001b[K\rremote: Compressing objects:  63% (157/249)\u001b[K\rremote: Compressing objects:  64% (160/249)\u001b[K\rremote: Compressing objects:  65% (162/249)\u001b[K\rremote: Compressing objects:  66% (165/249)\u001b[K\rremote: Compressing objects:  67% (167/249)\u001b[K\rremote: Compressing objects:  68% (170/249)\u001b[K\rremote: Compressing objects:  69% (172/249)\u001b[K\rremote: Compressing objects:  70% (175/249)\u001b[K\rremote: Compressing objects:  71% (177/249)\u001b[K\rremote: Compressing objects:  72% (180/249)\u001b[K\rremote: Compressing objects:  73% (182/249)\u001b[K\rremote: Compressing objects:  74% (185/249)\u001b[K\rremote: Compressing objects:  75% (187/249)\u001b[K\rremote: Compressing objects:  76% (190/249)\u001b[K\rremote: Compressing objects:  77% (192/249)\u001b[K\rremote: Compressing objects:  78% (195/249)\u001b[K\rremote: Compressing objects:  79% (197/249)\u001b[K\rremote: Compressing objects:  80% (200/249)\u001b[K\rremote: Compressing objects:  81% (202/249)\u001b[K\rremote: Compressing objects:  82% (205/249)\u001b[K\rremote: Compressing objects:  83% (207/249)\u001b[K\rremote: Compressing objects:  84% (210/249)\u001b[K\rremote: Compressing objects:  85% (212/249)\u001b[K\rremote: Compressing objects:  86% (215/249)\u001b[K\rremote: Compressing objects:  87% (217/249)\u001b[K\rremote: Compressing objects:  88% (220/249)\u001b[K\rremote: Compressing objects:  89% (222/249)\u001b[K\rremote: Compressing objects:  90% (225/249)\u001b[K\rremote: Compressing objects:  91% (227/249)\u001b[K\rremote: Compressing objects:  92% (230/249)\u001b[K\rremote: Compressing objects:  93% (232/249)\u001b[K\rremote: Compressing objects:  94% (235/249)\u001b[K\rremote: Compressing objects:  95% (237/249)\u001b[K\rremote: Compressing objects:  96% (240/249)\u001b[K\rremote: Compressing objects:  97% (242/249)\u001b[K\rremote: Compressing objects:  98% (245/249)\u001b[K\rremote: Compressing objects:  99% (247/249)\u001b[K\rremote: Compressing objects: 100% (249/249)\u001b[K\rremote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "Receiving objects:   0% (1/286)   \rReceiving objects:   1% (3/286)   \rReceiving objects:   2% (6/286)   \rReceiving objects:   3% (9/286)   \rReceiving objects:   4% (12/286)   \rReceiving objects:   5% (15/286)   \rReceiving objects:   6% (18/286)   \rReceiving objects:   7% (21/286)   \rReceiving objects:   8% (23/286)   \rReceiving objects:   9% (26/286)   \rReceiving objects:  10% (29/286)   \rReceiving objects:  11% (32/286)   \rReceiving objects:  12% (35/286)   \rReceiving objects:  13% (38/286)   \rReceiving objects:  14% (41/286)   \rReceiving objects:  15% (43/286)   \rReceiving objects:  16% (46/286)   \rReceiving objects:  17% (49/286)   \rReceiving objects:  18% (52/286)   \rReceiving objects:  19% (55/286)   \rReceiving objects:  20% (58/286)   \rReceiving objects:  21% (61/286)   \rReceiving objects:  22% (63/286)   \rReceiving objects:  23% (66/286)   \rReceiving objects:  24% (69/286)   \rReceiving objects:  25% (72/286)   \rReceiving objects:  26% (75/286)   \rReceiving objects:  27% (78/286)   \rReceiving objects:  28% (81/286)   \rReceiving objects:  29% (83/286)   \rReceiving objects:  30% (86/286)   \rReceiving objects:  31% (89/286)   \rReceiving objects:  32% (92/286)   \rReceiving objects:  33% (95/286)   \rReceiving objects:  34% (98/286)   \rReceiving objects:  35% (101/286)   \rReceiving objects:  36% (103/286)   \rReceiving objects:  37% (106/286)   \rReceiving objects:  38% (109/286)   \rReceiving objects:  39% (112/286)   \rReceiving objects:  40% (115/286)   \rReceiving objects:  41% (118/286)   \rReceiving objects:  42% (121/286)   \rReceiving objects:  43% (123/286)   \rReceiving objects:  44% (126/286)   \rReceiving objects:  45% (129/286)   \rReceiving objects:  46% (132/286)   \rReceiving objects:  47% (135/286)   \rReceiving objects:  48% (138/286)   \rReceiving objects:  49% (141/286)   \rReceiving objects:  50% (143/286)   \rReceiving objects:  51% (146/286)   \rReceiving objects:  52% (149/286)   \rReceiving objects:  53% (152/286)   \rReceiving objects:  54% (155/286)   \rReceiving objects:  55% (158/286)   \rReceiving objects:  56% (161/286)   \rReceiving objects:  57% (164/286)   \rReceiving objects:  58% (166/286)   \rReceiving objects:  59% (169/286)   \rReceiving objects:  60% (172/286)   \rReceiving objects:  61% (175/286)   \rReceiving objects:  62% (178/286)   \rReceiving objects:  63% (181/286)   \rReceiving objects:  64% (184/286)   \rReceiving objects:  65% (186/286)   \rReceiving objects:  66% (189/286)   \rReceiving objects:  67% (192/286)   \rReceiving objects:  68% (195/286)   \rReceiving objects:  69% (198/286)   \rReceiving objects:  70% (201/286)   \rReceiving objects:  71% (204/286)   \rReceiving objects:  72% (206/286)   \rReceiving objects:  73% (209/286)   \rReceiving objects:  74% (212/286)   \rReceiving objects:  75% (215/286)   \rReceiving objects:  76% (218/286)   \rReceiving objects:  77% (221/286)   \rReceiving objects:  78% (224/286)   \rReceiving objects:  79% (226/286)   \rReceiving objects:  80% (229/286)   \rReceiving objects:  81% (232/286)   \rReceiving objects:  82% (235/286)   \rReceiving objects:  83% (238/286)   \rReceiving objects:  84% (241/286)   \rReceiving objects:  85% (244/286)   \rReceiving objects:  86% (246/286)   \rReceiving objects:  87% (249/286)   \rReceiving objects:  88% (252/286)   \rReceiving objects:  89% (255/286)   \rReceiving objects:  90% (258/286)   \rReceiving objects:  91% (261/286)   \rReceiving objects:  92% (264/286)   \rremote: Total 286 (delta 67), reused 113 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects:  93% (266/286)   \rReceiving objects:  94% (269/286)   \rReceiving objects:  95% (272/286)   \rReceiving objects:  96% (275/286)   \rReceiving objects:  97% (278/286)   \rReceiving objects:  98% (281/286)   \rReceiving objects:  99% (284/286)   \rReceiving objects: 100% (286/286)   \rReceiving objects: 100% (286/286), 701.75 KiB | 2.99 MiB/s, done.\n",
            "Resolving deltas:   0% (0/67)   \rResolving deltas:   1% (1/67)   \rResolving deltas:   2% (2/67)   \rResolving deltas:  16% (11/67)   \rResolving deltas:  19% (13/67)   \rResolving deltas:  23% (16/67)   \rResolving deltas:  29% (20/67)   \rResolving deltas:  32% (22/67)   \rResolving deltas:  38% (26/67)   \rResolving deltas:  41% (28/67)   \rResolving deltas:  44% (30/67)   \rResolving deltas:  46% (31/67)   \rResolving deltas:  47% (32/67)   \rResolving deltas:  49% (33/67)   \rResolving deltas:  52% (35/67)   \rResolving deltas:  64% (43/67)   \rResolving deltas:  71% (48/67)   \rResolving deltas:  77% (52/67)   \rResolving deltas:  82% (55/67)   \rResolving deltas:  94% (63/67)   \rResolving deltas: 100% (67/67)   \rResolving deltas: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfDRa8tGeFCa",
        "colab_type": "code",
        "outputId": "5a06b8e9-67ca-4663-89d4-35696391ec82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd faceswap/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/faceswap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K52YSCrseIA3",
        "colab_type": "code",
        "outputId": "bdacab2b-0361-4af3-a273-97c6fb036d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python setup.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO   \u001b[0m Running as Root/Admin\n",
            "\u001b[32mINFO   \u001b[0m The tool provides tips for installation\r\n",
            "        and installs required python packages\n",
            "\u001b[32mINFO   \u001b[0m Setup in Linux 4.19.104+\n",
            "\u001b[32mINFO   \u001b[0m Installed Python: 3.6.9 64bit\n",
            "\u001b[32mINFO   \u001b[0m Encoding: UTF-8\n",
            "\u001b[32mINFO   \u001b[0m Upgrading pip...\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.0MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installed pip: 19.3.1\n",
            "\u001b[32mINFO   \u001b[0m AMD Support: AMD GPU support is currently limited.\n",
            "        Nvidia Users MUST answer 'no' to this option.\n",
            "Enable AMD Support? [y/N] n\n",
            "\u001b[32mINFO   \u001b[0m AMD Support Disabled\n",
            "Enable  Docker? [y/N] n\n",
            "\u001b[32mINFO   \u001b[0m Docker Disabled\n",
            "Enable  CUDA? [Y/n] y\n",
            "\u001b[32mINFO   \u001b[0m CUDA Enabled\n",
            "\u001b[32mINFO   \u001b[0m CUDA version: 10.1\n",
            "\u001b[32mINFO   \u001b[0m cuDNN version: 7.6.5\n",
            "\u001b[33mWARNING\u001b[0m The minimum Tensorflow requirement is 1.12. \n",
            "        Tensorflow currently has no official prebuild for your CUDA, cuDNN combination.\n",
            "        Either install a combination that Tensorflow supports or build and install your own tensorflow-gpu.\n",
            "        CUDA Version: 10.1\n",
            "        cuDNN Version: 7.6\n",
            "        Help:\n",
            "        Building Tensorflow: https://www.tensorflow.org/install/install_sources\n",
            "        Tensorflow supported versions: https://www.tensorflow.org/install/source#tested_build_configurations\n",
            "Location of custom tensorflow-gpu wheel (leave blank to manually install): \n",
            "\u001b[32mINFO   \u001b[0m Faceswap config written to: /content/faceswap/config/.faceswap\n",
            "Please ensure your System Dependencies are met. Continue? [y/N] y\n",
            "\u001b[32mINFO   \u001b[0m Installing Required Python Packages. This may take some time...\n",
            "\u001b[32mINFO   \u001b[0m Installing numpy==1.17.4\n",
            "\u001b[K     |████████████████████████████████| 20.0 MB 1.1 MB/s \n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing Pillow==6.2.1\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.1 MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing toposort\n",
            "\u001b[32mINFO   \u001b[0m Installing fastcluster\n",
            "\u001b[K     |████████████████████████████████| 154 kB 4.6 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing matplotlib==3.1.1\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 4.1 MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing imageio==2.6.1\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.0 MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing imageio-ffmpeg\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.2 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing ffmpy==0.2.2\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[32mINFO   \u001b[0m Installing git+https://github.com/deepfakes/nvidia-ml-py3.git\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[32mINFO   \u001b[0m Installing h5py==2.9.0\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 4.8 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing Keras==2.2.4\n",
            "\u001b[K     |████████████████████████████████| 312 kB 4.8 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m All python3 dependencies are met.\n",
            "        You are good to go.\n",
            "        \n",
            "        Enter:  'python faceswap.py -h' to see the options\n",
            "                'python faceswap.py gui' to launch the GUI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12af5iKGeKW-",
        "colab_type": "code",
        "outputId": "7f2ced74-4ced-4a99-b9d4-0fa300a16cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "!python faceswap.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "usage: faceswap.py [-h] {extract,train,convert,gui} ...\n",
            "\n",
            "positional arguments:\n",
            "  {extract,train,convert,gui}\n",
            "    extract             Extract the faces from pictures\n",
            "    train               This command trains the model for the two faces A and\n",
            "                        B\n",
            "    convert             Convert a source image to a new one with the face\n",
            "                        swapped\n",
            "    gui                 Launch the Faceswap Graphical User Interface\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNABfP_HekJN",
        "colab_type": "code",
        "outputId": "3f1fc120-2195-44fa-e915-f88c3647a816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python faceswap.py extract -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "usage: faceswap.py extract [-h] [-C CONFIGFILE]\n",
            "                           [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE] -i\n",
            "                           INPUT_DIR -o OUTPUT_DIR [-al ALIGNMENTS_PATH]\n",
            "                           [-D {cv2-dnn,mtcnn,s3fd}] [-A {cv2-dnn,fan}]\n",
            "                           [-M {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...]]\n",
            "                           [-nm {none,clahe,hist,mean}] [-r ROTATE_IMAGES]\n",
            "                           [-min MIN_SIZE] [-n NFILTER [NFILTER ...]]\n",
            "                           [-f FILTER [FILTER ...]] [-l REF_THRESHOLD]\n",
            "                           [-een EXTRACT_EVERY_N] [-sz SIZE]\n",
            "                           [-si SAVE_INTERVAL] [-dl] [-sp] [-s] [-sf]\n",
            "\n",
            "Extract the faces from pictures\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -C CONFIGFILE, --configfile CONFIGFILE\n",
            "                        Optionally overide the saved config with the path to a\n",
            "                        custom config file.\n",
            "  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}\n",
            "                        Log level. Stick with INFO or VERBOSE unless you need\n",
            "                        to file an error report. Be careful with TRACE as it\n",
            "                        will generate a lot of data\n",
            "  -LF LOGFILE, --logfile LOGFILE\n",
            "                        Path to store the logfile. Leave blank to store in the\n",
            "                        faceswap folder\n",
            "  -i INPUT_DIR, --input-dir INPUT_DIR\n",
            "                        Input directory or video. Either a directory\n",
            "                        containing the image files you wish to process or path\n",
            "                        to a video file. NB: This should be the source\n",
            "                        video/frames NOT the source faces.\n",
            "  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
            "                        Output directory. This is where the converted files\n",
            "                        will be saved.\n",
            "  -al ALIGNMENTS_PATH, --alignments ALIGNMENTS_PATH\n",
            "                        Optional path to an alignments file. Leave blank if\n",
            "                        the alignments file is at the default location.\n",
            "  -D {cv2-dnn,mtcnn,s3fd}, --detector {cv2-dnn,mtcnn,s3fd}\n",
            "                        Detector to use. Some of these have configurable\n",
            "                        settings in '/config/extract.ini' or 'Settings >\n",
            "                        Configure Extract 'Plugins':\n",
            "                          - cv2-dnn: A CPU only extractor which is the least\n",
            "                            reliable and least resource intensive. Use this if\n",
            "                            not using a GPU and time is important.\n",
            "                          - mtcnn: Good detector. Fast on CPU, faster on GPU.\n",
            "                            Uses fewer resources than other GPU detectors but\n",
            "                            can often return more false positives.\n",
            "                          - s3fd: Best detector. Fast on GPU, slow on CPU. Can\n",
            "                            detect more faces and fewer false positives than\n",
            "                            other GPU detectors, but is a lot more resource\n",
            "                            intensive.\n",
            "  -A {cv2-dnn,fan}, --aligner {cv2-dnn,fan}\n",
            "                        Aligner to use.\n",
            "                          - cv2-dnn: A CPU only landmark detector. Faster,\n",
            "                            less resource intensive, but less accurate. Only\n",
            "                            use this if not using a GPU and time is important.\n",
            "                          - fan: Best aligner. Fast on GPU, slow on CPU.\n",
            "  -M {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...], --masker {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...]\n",
            "                        Additional Masker(s) to use. The masks generated here\n",
            "                        will all take up GPU RAM. You can select none, one or\n",
            "                        multiple masks, but the extraction may take longer the\n",
            "                        more you select. NB: The Extended and Components\n",
            "                        (landmark based) masks are automatically generated on\n",
            "                        extraction.\n",
            "                          - vgg-clear: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces clear of\n",
            "                            obstructions. Profile faces and obstructions may\n",
            "                            result in sub-par performance.\n",
            "                          - vgg-obstructed: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been specifically trained to recognize\n",
            "                            some facial obstructions (hands and eyeglasses).\n",
            "                            Profile faces may result in sub-par performance.\n",
            "                          - unet-dfl: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been trained by community members and\n",
            "                            will need testing for further description. Profile\n",
            "                            faces may result in sub-par performance.\n",
            "                        The auto generated masks are as follows:\n",
            "                          - components: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks to create a mask.\n",
            "                          - extended: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks and the mask is extended\n",
            "                            upwards onto the forehead.\n",
            "                        (eg: `-M unet-dfl vgg-clear`, `--masker vgg-\n",
            "                        obstructed`)\n",
            "  -nm {none,clahe,hist,mean}, --normalization {none,clahe,hist,mean}\n",
            "                        Performing normalization can help the aligner better\n",
            "                        align faces with difficult lighting conditions at an\n",
            "                        extraction speed cost. Different methods will yield\n",
            "                        different results on different sets. NB: This does not\n",
            "                        impact the output face, just the input to the aligner.\n",
            "                          - none: Don't perform normalization on the face.\n",
            "                          - clahe: Perform Contrast Limited Adaptive Histogram\n",
            "                            Equalization on the face.\n",
            "                          - hist: Equalize the histograms on the RGB channels.\n",
            "                          - mean: Normalize the face colors to the mean.\n",
            "  -r ROTATE_IMAGES, --rotate-images ROTATE_IMAGES\n",
            "                        If a face isn't found, rotate the images to try to\n",
            "                        find a face. Can find more faces at the cost of\n",
            "                        extraction speed. Pass in a single number to use\n",
            "                        increments of that size up to 360, or pass in a list\n",
            "                        of numbers to enumerate exactly what angles to check.\n",
            "  -min MIN_SIZE, --min-size MIN_SIZE\n",
            "                        Filters out faces detected below this size. Length, in\n",
            "                        pixels across the diagonal of the bounding box. Set to\n",
            "                        0 for off\n",
            "  -n NFILTER [NFILTER ...], --nfilter NFILTER [NFILTER ...]\n",
            "                        Optionally filter out people who you do not wish to\n",
            "                        process by passing in an image of that person. Should\n",
            "                        be a front portrait with a single person in the image.\n",
            "                        Multiple images can be added space separated. NB:\n",
            "                        Using face filter will significantly decrease\n",
            "                        extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -f FILTER [FILTER ...], --filter FILTER [FILTER ...]\n",
            "                        Optionally select people you wish to process by\n",
            "                        passing in an image of that person. Should be a front\n",
            "                        portrait with a single person in the image. Multiple\n",
            "                        images can be added space separated. NB: Using face\n",
            "                        filter will significantly decrease extraction speed\n",
            "                        and its accuracy cannot be guaranteed.\n",
            "  -l REF_THRESHOLD, --ref_threshold REF_THRESHOLD\n",
            "                        For use with the optional nfilter/filter files.\n",
            "                        Threshold for positive face recognition. Lower values\n",
            "                        are stricter. NB: Using face filter will significantly\n",
            "                        decrease extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -een EXTRACT_EVERY_N, --extract-every-n EXTRACT_EVERY_N\n",
            "                        Extract every 'nth' frame. This option will skip\n",
            "                        frames when extracting faces. For example a value of 1\n",
            "                        will extract faces from every frame, a value of 10\n",
            "                        will extract faces from every 10th frame.\n",
            "  -sz SIZE, --size SIZE\n",
            "                        The output size of extracted faces. Make sure that the\n",
            "                        model you intend to train supports your required size.\n",
            "                        This will only need to be changed for hi-res models.\n",
            "  -si SAVE_INTERVAL, --save-interval SAVE_INTERVAL\n",
            "                        Automatically save the alignments file after a set\n",
            "                        amount of frames. By default the alignments file is\n",
            "                        only saved at the end of the extraction process. NB:\n",
            "                        If extracting in 2 passes then the alignments file\n",
            "                        will only start to be saved out during the second\n",
            "                        pass. WARNING: Don't interrupt the script when writing\n",
            "                        the file because it might get corrupted. Set to 0 to\n",
            "                        turn off\n",
            "  -dl, --debug-landmarks\n",
            "                        Draw landmarks on the ouput faces for debugging\n",
            "                        purposes.\n",
            "  -sp, --singleprocess  Don't run extraction in parallel. Will run each part\n",
            "                        of the extraction process separately (one after the\n",
            "                        other) rather than all at the smae time. Useful if\n",
            "                        VRAM is at a premium.\n",
            "  -s, --skip-existing   Skips frames that have already been extracted and\n",
            "                        exist in the alignments file\n",
            "  -sf, --skip-existing-faces\n",
            "                        Skip frames that already have detected faces in the\n",
            "                        alignments file\n",
            "\n",
            "Questions and feedback: https://faceswap.dev/forum\n",
            "faceswap.py extract: error: the following arguments are required: -i/--input-dir, -o/--output-dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSiJcbfljuhD",
        "colab_type": "code",
        "outputId": "692a78cd-6f2b-4487-f605-1193aacad5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "!python faceswap.py extract -i video1.mp4 -o extract_video "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "04/27/2020 04:31:15 INFO     Log level set to: INFO\n",
            "04/27/2020 04:31:17 INFO     Output Directory: /content/faceswap/extract_video\n",
            "04/27/2020 04:31:18 INFO     Loading Detect from S3Fd plugin...\n",
            "Using TensorFlow backend.\n",
            "04/27/2020 04:31:18 INFO     Loading Align from Fan plugin...\n",
            "04/27/2020 04:31:18 INFO     Loading Mask from Components plugin...\n",
            "04/27/2020 04:31:18 INFO     Loading Mask from Extended plugin...\n",
            "04/27/2020 04:31:18 INFO     Starting, this may take a while...\n",
            "04/27/2020 04:31:18 INFO     Initializing S3FD (Detect)...\n",
            "04/27/2020 04:31:23 INFO     Initialized S3FD (Detect) with batchsize of 4\n",
            "04/27/2020 04:31:23 INFO     Initializing FAN (Align)...\n",
            "04/27/2020 04:31:46 INFO     Initialized FAN (Align) with batchsize of 12\n",
            "04/27/2020 04:31:46 INFO     Initializing Components (Mask)...\n",
            "04/27/2020 04:31:46 INFO     Initialized Components (Mask) with batchsize of 1\n",
            "04/27/2020 04:31:46 INFO     Initializing Extended (Mask)...\n",
            "04/27/2020 04:31:46 INFO     Initialized Extended (Mask) with batchsize of 1\n",
            "Running pass 1 of 1: Extraction: 100% 44280/44280 [1:06:57<00:00, 11.02it/s]\n",
            "04/27/2020 05:38:43 INFO     Writing alignments to: '/content/faceswap/pual_alignments.fsa'\n",
            "04/27/2020 05:38:50 INFO     -------------------------\n",
            "04/27/2020 05:38:50 INFO     Images found:        44280\n",
            "04/27/2020 05:38:50 INFO     Faces detected:      80935\n",
            "04/27/2020 05:38:50 INFO     -------------------------\n",
            "04/27/2020 05:38:50 INFO     Note:\n",
            "04/27/2020 05:38:50 INFO     Multiple faces were detected in one or more pictures.\n",
            "04/27/2020 05:38:50 INFO     Double check your results.\n",
            "04/27/2020 05:38:50 INFO     -------------------------\n",
            "04/27/2020 05:38:50 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgUW7qjtLifa",
        "colab_type": "code",
        "outputId": "da1c0867-52dc-45d5-dbec-237222e2bbf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "!python faceswap.py extract -i video2.mp4 -o extract_video2 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "04/27/2020 05:49:06 INFO     Log level set to: INFO\n",
            "04/27/2020 05:49:11 INFO     Output Directory: /content/faceswap/extract_video2\n",
            "04/27/2020 05:49:11 INFO     Loading Detect from S3Fd plugin...\n",
            "Using TensorFlow backend.\n",
            "04/27/2020 05:49:11 INFO     Loading Align from Fan plugin...\n",
            "04/27/2020 05:49:11 INFO     Loading Mask from Components plugin...\n",
            "04/27/2020 05:49:11 INFO     Loading Mask from Extended plugin...\n",
            "04/27/2020 05:49:12 INFO     Starting, this may take a while...\n",
            "04/27/2020 05:49:12 INFO     Initializing S3FD (Detect)...\n",
            "04/27/2020 05:49:16 INFO     Initialized S3FD (Detect) with batchsize of 4\n",
            "04/27/2020 05:49:16 INFO     Initializing FAN (Align)...\n",
            "04/27/2020 05:49:39 INFO     Initialized FAN (Align) with batchsize of 12\n",
            "04/27/2020 05:49:39 INFO     Initializing Components (Mask)...\n",
            "04/27/2020 05:49:39 INFO     Initialized Components (Mask) with batchsize of 1\n",
            "04/27/2020 05:49:39 INFO     Initializing Extended (Mask)...\n",
            "04/27/2020 05:49:39 INFO     Initialized Extended (Mask) with batchsize of 1\n",
            "Running pass 1 of 1: Extraction: 100% 6755/6755 [32:43<00:00,  3.44it/s]\n",
            "04/27/2020 06:22:23 INFO     Writing alignments to: '/content/faceswap/obama_alignments.fsa'\n",
            "04/27/2020 06:22:25 INFO     -------------------------\n",
            "04/27/2020 06:22:25 INFO     Images found:        6755\n",
            "04/27/2020 06:22:25 INFO     Faces detected:      27062\n",
            "04/27/2020 06:22:25 INFO     -------------------------\n",
            "04/27/2020 06:22:25 INFO     Note:\n",
            "04/27/2020 06:22:25 INFO     Multiple faces were detected in one or more pictures.\n",
            "04/27/2020 06:22:25 INFO     Double check your results.\n",
            "04/27/2020 06:22:25 INFO     -------------------------\n",
            "04/27/2020 06:22:25 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8L4SaVqLmcP",
        "colab_type": "code",
        "outputId": "5ba4f17f-a1f9-4d59-8590-665fc1a135eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "!python faceswap.py train -A extract_video -B extract_video2 -m Model_data_new_videos"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "04/27/2020 06:31:33 INFO     Log level set to: INFO\n",
            "Using TensorFlow backend.\n",
            "04/27/2020 06:31:36 INFO     Model A Directory: /content/faceswap/extract_video\n",
            "04/27/2020 06:31:36 INFO     Model B Directory: /content/faceswap/extract_video2\n",
            "04/27/2020 06:31:36 INFO     Training data directory: /content/faceswap/Model_data_new_videos\n",
            "04/27/2020 06:31:36 INFO     ===================================================\n",
            "04/27/2020 06:31:36 INFO       Starting\n",
            "04/27/2020 06:31:36 INFO       Press 'ENTER' to save and quit\n",
            "04/27/2020 06:31:36 INFO       Press 'S' to save model weights immediately\n",
            "04/27/2020 06:31:36 INFO     ===================================================\n",
            "04/27/2020 06:31:37 INFO     Loading data, this may take a while...\n",
            "04/27/2020 06:31:37 INFO     Loading Model from Original plugin...\n",
            "04/27/2020 06:31:37 INFO     No existing state file found. Generating.\n",
            "04/27/2020 06:31:38 INFO     Creating new 'original' model in folder: '/content/faceswap/Model_data_new_videos'\n",
            "04/27/2020 06:31:38 INFO     Loading Trainer from Original plugin...\n",
            "04/27/2020 06:31:40 INFO     Enabled TensorBoard Logging\n",
            "[06:31:51] [#00001] Loss A: 0.17465, Loss B: 0.24960\n",
            "04/27/2020 06:31:51 INFO     Backing up models...\n",
            "04/27/2020 06:31:52 INFO     [Saved models] - Average since last save: face_loss_A: 0.17465, face_loss_B: 0.24960\n",
            "[06:33:11] [#00101] Loss A: 0.11991, Loss B: 0.11955\n",
            "04/27/2020 06:33:11 INFO     Backing up models...\n",
            "04/27/2020 06:33:12 INFO     [Saved models] - Average since last save: face_loss_A: 0.14467, face_loss_B: 0.16751\n",
            "[06:34:31] [#00201] Loss A: 0.08893, Loss B: 0.08733\n",
            "04/27/2020 06:34:31 INFO     Backing up models...\n",
            "04/27/2020 06:34:32 INFO     [Saved models] - Average since last save: face_loss_A: 0.10509, face_loss_B: 0.11226\n",
            "[06:35:53] [#00301] Loss A: 0.11696, Loss B: 0.10822\n",
            "04/27/2020 06:35:53 INFO     Backing up models...\n",
            "04/27/2020 06:35:54 INFO     [Saved models] - Average since last save: face_loss_A: 0.08696, face_loss_B: 0.09015\n",
            "[06:36:59] [#00382] Loss A: 0.07453, Loss B: 0.0783804/27/2020 06:36:59 INFO     Exit requested! The trainer will complete its current cycle, save the models and quit (This can take a couple of minutes depending on your training speed). If you want to kill it now, press Ctrl + c\n",
            "[06:36:59] [#00383] Loss A: 0.08931, Loss B: 0.07343\n",
            "04/27/2020 06:36:59 INFO     Backing up models...\n",
            "04/27/2020 06:37:00 INFO     [Saved models] - Average since last save: face_loss_A: 0.07676, face_loss_B: 0.07537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lTmjTUdVW0v",
        "colab_type": "code",
        "outputId": "b1bd305c-2b92-419e-f315-4dae93a05c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "!python faceswap.py convert -i video1.mp4 -o outpute_dir_video -m \"/content/faceswap/Model_data_new_videos\" -w ffmpeg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "04/27/2020 07:26:09 INFO     Log level set to: INFO\n",
            "Using TensorFlow backend.\n",
            "04/27/2020 07:26:12 INFO     Reading alignments from: '/content/faceswap/pual_alignments.fsa'\n",
            "04/27/2020 07:26:14 INFO     Loading Writer from Ffmpeg plugin...\n",
            "04/27/2020 07:26:14 INFO     Loading Model from Original plugin...\n",
            "04/27/2020 07:26:14 INFO     Using configuration saved in state file\n",
            "04/27/2020 07:26:32 INFO     Loaded model from disk: '/content/faceswap/Model_data_new_videos'\n",
            "04/27/2020 07:26:32 INFO     Loading Mask from Box_Blend plugin...\n",
            "04/27/2020 07:26:32 INFO     Loading Mask from Mask_Blend plugin...\n",
            "04/27/2020 07:26:32 INFO     Loading Color from Avg_Color plugin...\n",
            "04/27/2020 07:26:38 INFO     Outputting to: '/content/faceswap/outpute_dir_video/pual_converted.mp4'\n",
            "04/27/2020 07:26:39 WARNING  IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=8, resizing from (450, 360) to (456, 360) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "Converting: 100% 44280/44280 [21:36<00:00, 34.15it/s]\n",
            "04/27/2020 07:47:51 INFO     Muxing Audio...\n",
            "04/27/2020 07:47:52 INFO     -------------------------\n",
            "04/27/2020 07:47:52 INFO     Images found:        44280\n",
            "04/27/2020 07:47:52 INFO     Faces detected:      80935\n",
            "04/27/2020 07:47:52 INFO     -------------------------\n",
            "04/27/2020 07:47:52 INFO     Note:\n",
            "04/27/2020 07:47:52 INFO     Multiple faces were detected in one or more pictures.\n",
            "04/27/2020 07:47:52 INFO     Double check your results.\n",
            "04/27/2020 07:47:52 INFO     -------------------------\n",
            "04/27/2020 07:47:52 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}